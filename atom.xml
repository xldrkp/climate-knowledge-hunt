<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title>#semanticClimate</title>
	<subtitle>Liberating knowledge from climate-related reports</subtitle>
	
	<link href="/p/atom.xml" rel="self"/>
	<link href=""/>
	<updated>2023-08-14T07:16:57Z</updated>
	<id>https://semanticclimate.github.io/</id>
	<author>
		<name>#semanticClimate</name>
		<email></email>
	</author>
	
	<entry>
		<title>Testpost</title>
		<link href="/p/en/posts/testpost/"/>
		<updated>2023-08-04T00:00:00Z</updated>
		<id>/p/en/posts/testpost/</id>
		<content type="html">&lt;p&gt;This is a post that is hidden in the timeline of posts due to the flag &lt;code&gt;hide: true&lt;/code&gt; in the YAML frontmatter. It has a regular URL and can thus be linked to.&lt;/p&gt;
&lt;p&gt;For the logic related to the flag please cf. &lt;code&gt;_includes/components/postslist.njk&lt;/code&gt; and the wide &lt;code&gt;if&lt;/code&gt; statement from line 4 to 33.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Alpha Tests for FSCI 2023</title>
		<link href="/p/en/posts/alpha-tests-for-fsci-23/"/>
		<updated>2023-08-02T00:00:00Z</updated>
		<id>/p/en/posts/alpha-tests-for-fsci-23/</id>
		<content type="html">&lt;h1&gt;Pre-requisite&lt;/h1&gt;
&lt;p&gt;All of our tools are run written in Python and need to be run from the Commandline.&lt;/p&gt;
&lt;h2&gt;Install Python and Pip (Instruction Credits: Waheb Mehdi)&lt;/h2&gt;
&lt;h3&gt;1. Downloading Python from www.python.org&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&quot;www.python.org&quot;&gt;www.python.org&lt;/a&gt; and hover over the &amp;quot;Downloads&amp;quot; tab.&lt;/li&gt;
&lt;li&gt;Click on the option to download Python (we recommend version 3.8.) for your Windows system.&lt;/li&gt;
&lt;li&gt;Once the file is downloaded, double click on it to launch the Python for Windows setup.&lt;/li&gt;
&lt;li&gt;In the Python setup dialog box, make sure to check the option &amp;quot;Add python.exe to PATH&amp;quot; to enable accessing Python from the command line.&lt;/li&gt;
&lt;li&gt;Click on &amp;quot;Install now&amp;quot; and wait for a few minutes while Python setup completes the installation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Verify Python installation&lt;/h3&gt;
&lt;p&gt;Open your command line interface (CMD or PowerShell).
Type either of the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;py --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command should display the version of Python installed on your machine and verify the successful installation.&lt;/p&gt;
&lt;h3&gt;Installing pip&lt;/h3&gt;
&lt;p&gt;Pip is a package manager in Python that helps you download packages from the Python Package Index (PyPi).&lt;/p&gt;
&lt;p&gt;Newer versions of Python (starting from Python 3.4) come with pip preinstalled. If you&#39;re running an older version, follow these steps to manually install pip from the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m ensurepip --upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ensure that pip was installed correctly, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip --version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the installation was successful, this command will display the version number of pip.&lt;/p&gt;
&lt;h1&gt;Test 01&lt;/h1&gt;
&lt;p&gt;It would be very useful if you could test whether our PDF to HTML tool pyamihtml can be installed. Please note any crashes or garbles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install pyamihtml==0.0.7a1
pyamihtml IPCC --help 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That should take a minute or so (more if you don&#39;t have some of the libraries installed) and give a commandlne help message.&lt;/p&gt;
&lt;h1&gt;Test 02A&lt;/h1&gt;
&lt;p&gt;Alpha Test for experienced installers (with pyamihtml)&lt;/p&gt;
&lt;p&gt;If you have installed pyamihtml (see TEST 01) then please try to run it on  @yasin&#39;s Chapter05 (114 pages of PDF). You will read directly from the IPCC site (so need to be connected). You need a directory to store the results (here I create one under my HOME directory, but you can use anything that makes sense). This will probably take 30-60 seconds - depends mainly on download speeds.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pyamihtml IPCC --input https://www.ipcc.ch/site/assets/uploads/sites/4/2022/11/SRCCL_Chapter_5.pdf --outdir $HOME/temp/ar6/srccl/chapter05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please let us know how you get on. A satisfactory run starts like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;==============PAGE 1================
writing XML /Users/pm286/temp/ar6/srccl/chapter05/page_1.html
==============PAGE 2================
writing XML /Users/pm286/temp/ar6/srccl/chapter05/page_2.html
==============PAGE 3================
and ends like:
==============PAGE 114================
writing XML /Users/pm286/temp/ar6/srccl/chapter05/page_114.html
writing XML /Users/pm286/temp/ar6/srccl/chapter05/total_pages.html
total_pages elems: 1661
total_pages content 1661
wrote: /Users/pm286/temp/ar6/srccl/chapter05/styles1.html
wrote: /Users/pm286/temp/ar6/srccl/chapter05/groups_styles.html
wrote: /Users/pm286/temp/ar6/srccl/chapter05/groups_groups.html
wrote: /Users/pm286/temp/ar6/srccl/chapter05/groups_statements.html
after sections: 0
debug divs: 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;View `Users/pm286/temp/ar6/srccl/chapter05/groups_groups.html``
in a browser and verify it makes sense.&lt;/p&gt;
&lt;h1&gt;Test 02B&lt;/h1&gt;
&lt;p&gt;Alpha Test for experienced installers (with &lt;code&gt;pyamihtml&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;As for TEST2a (see replies above) but use
@shiwani’s chapter (High mountains) https://www.ipcc.ch/site/assets/uploads/sites/3/2022/03/04_SROCC_Ch02_FINAL.pdf
and output &lt;code&gt;$HOME/temp/ar6/srocc/chapter02&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pyamihtml IPCC --input https://www.ipcc.ch/site/assets/uploads/sites/3/2022/03/04_SROCC_Ch02_FINAL.pdf --outdir $HOME/temp/ar6/srocc/chapter02
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Test 02C&lt;/h1&gt;
&lt;p&gt;Alpha Test for experienced installers (with pyamihtml)&lt;/p&gt;
&lt;p&gt;Use the team&#39;s chapter on urban systems (cities) https://www.ipcc.ch/report/ar6/wg3/downloads/report/IPCC_AR6_WGIII_Chapter08.pdf and output &lt;code&gt;$HOME/temp/ar6/wg3/chapter08&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pyamihtml IPCC --input https://www.ipcc.ch/report/ar6/wg3/downloads/report/IPCC_AR6_WGIII_Chapter08.pdf --outdir $HOME/temp/ar6/wg3/chapter08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: The original chapters came out last year in draft form but this year were replaced by final reports. Make sure you use the later ones (our repo contains both.&lt;/p&gt;
&lt;h1&gt;Test 03&lt;/h1&gt;
&lt;p&gt;Alpha Test for experienced installers (with pygetpapers)&lt;/p&gt;
&lt;p&gt;Ayush Garg joined semanticClimate 3 years ago when he was 15! He and I took Rik Smith-Unna&#39;s getpapers and Pythonised it. It lets you search the scholarly literature and download it automatically. This tool searches repositories with APIs. The default is europepmc.org , a collection of all Open biomedical data (this is interpreted very widely and increasingly include maths, social sciences, etc.) It&#39;s the go-to place for an initial scoping of the literature.
pygetpapers wraps a traditional boolean text query (with metadata, dates, access, etc) and submits it to EPMC. It receives the hits as a JSON file, downloads it and then systematically retrieves hits as (a) PDF (b) XML.&lt;/p&gt;
&lt;p&gt;An example for recent climate change papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first install pygetpapers&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;pip install pygetpapers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then run the search. This is &amp;quot;climate change&amp;quot; published in 2023. The command will search EuropePMC, download 10 papers, with PDF and XML to a named directory. It takes a bit of time as we done&#39;t want to overload the server. If it works expand to a few more&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pygetpapers -q &amp;quot;climate crisis&amp;quot; --startdate 2023 -p -x -k 10 -o ~/temp/climate_crisis
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Test 04&lt;/h1&gt;
&lt;p&gt;Now we have got HTML/XML after running either &lt;code&gt;pygetpapers&lt;/code&gt; on a repository or &lt;code&gt;pyamihtml&lt;/code&gt; on a local corpus. To extract the science we use docanalysis written by Shweata Hegde and Ayush Garg.&lt;/p&gt;
&lt;p&gt;This searches the corpus in either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;supervised mode where we have lists of words that might occur in the text OR&lt;/li&gt;
&lt;li&gt;unsupervised keyword/keyphrase extraction where language processing models find phrases that are statistically significant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next phase is to link the terms to Wikidata. If you have never used WD think of it of all the data in Wikipedia, and then a lot more. over 100,000,000 pieces of data. Many of these are climate terms, so we can link to WD (of course it&#39;s Open). The next phase is to analyse the search results. We will probably do this with Pandas - the Python data analysis/presentation tool.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE: docanalysis needs Python &amp;lt;= 3.8&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install docanalysis. See https://pypi.org/project/docanalysis/ (`pip install docanalysis``)
test it runs:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docanalysis
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;... gives me ...&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docanalysis
/opt/anaconda3/lib/python3.8/site-packages/_distutils_hack/__init__.py:36: UserWarning: Setuptools is replacing distutils.
  warnings.warn(&amp;quot;Setuptools is replacing distutils.&amp;quot;)
usage: docanalysis [-h] [--run_pygetpapers] [--make_section] [-q QUERY] [-k HITS] [--project_name PROJECT_NAME]
                   [-d [DICTIONARY [DICTIONARY ...]]] [-o OUTPUT] [--make_ami_dict MAKE_AMI_DICT]
                   [--search_section [SEARCH_SECTION [SEARCH_SECTION ...]]] [--entities [ENTITIES [ENTITIES ...]]]
                   [--spacy_model SPACY_MODEL] [--html HTML] [--synonyms SYNONYMS] [--make_json MAKE_JSON] [--search_html]
                   [--extract_abb EXTRACT_ABB] [-l LOGLEVEL] [-f LOGFILE]

Welcome to docanalysis version 0.1.9. -h or --help for help

optional arguments:
  -h, --help            show this help message and exit
  --run_pygetpapers     [Command] downloads papers from EuropePMC via pygetpapers
  --make_section        [Command] makes sections; requires a fulltext.xml in CTree directories
  -q QUERY, --query QUERY
                        [pygetpapers] query string
  -k HITS, --hits HITS  [pygetpapers] number of papers to download
  --project_name PROJECT_NAME
                        CProject directory name
  -d [DICTIONARY [DICTIONARY ...]], --dictionary [DICTIONARY [DICTIONARY ...]]
                        [file name/url] existing ami dictionary to annotate sentences or support supervised entity
                        extraction
  -o OUTPUT, --output OUTPUT
                        outputs csv with sentences/terms
  --make_ami_dict MAKE_AMI_DICT
                        [Command] title for ami-dict. Makes ami-dict of all extracted entities; works only with spacy
  --search_section [SEARCH_SECTION [SEARCH_SECTION ...]]
                        [NER/dictionary search] section(s) to annotate. Choose from: ALL, ACK, AFF, AUT, CON, DIS, ETH,
                        FIG, INT, KEY, MET, RES, TAB, TIL. Defaults to ALL
  --entities [ENTITIES [ENTITIES ...]]
                        [NER] entities to extract. Default (ALL). Common entities SpaCy: GPE, LANGUAGE, ORG, PERSON (for
                        additional ones check: ); SciSpaCy: CHEMICAL, DISEASE
  --spacy_model SPACY_MODEL
                        [NER] optional. Choose between spacy or scispacy models. Defaults to spacy
  --html HTML           outputs html with sentences/terms
  --synonyms SYNONYMS   annotate the corpus/sections with synonyms from ami-dict
  --make_json MAKE_JSON
                        outputs json with sentences/terms
  --search_html         searches html documents (mainly IPCC)
  --extract_abb EXTRACT_ABB
                        [Command] title for abb-ami-dict. Extracts abbreviations and expansions; makes ami-dict of all
                        extracted entities
  -l LOGLEVEL, --loglevel LOGLEVEL
                        provide logging level. Example --log warning &amp;lt;&amp;lt;info,warning,debug,error,critical&amp;gt;&amp;gt;, default=&#39;info&#39;
  -f LOGFILE, --logfile LOGFILE
                        saves log to specified file in output directory as well as printing to terminal

&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Test 4A&lt;/h1&gt;
&lt;p&gt;Recent papers on climate crisis and floods in Ladakh.&lt;/p&gt;
&lt;p&gt;You have to be careful with quotes as we have phrases with spaces&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pygetpapers -q &amp;quot;&#39;climate crisis&#39; AND &#39;Ladakh&#39;&amp;quot; --startdate 2021  -n 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO: Total number of hits for the query are 19
That&#39;s a good number to work with. It will go up to any number - the default is 100.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we save them:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pygetpapers -q &amp;quot;&#39;climate crisis&#39; AND &#39;Ladakh&#39;&amp;quot; --startdate 2021  -x -p -o ladakh_2021
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;takes 45 sec.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;docanalysis&lt;/code&gt; to section the papers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; docanalysis --project_name ladakh_2021 --make_section
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a sophisticated search but it takes a bit of practice. This will search ALL sections in &lt;code&gt;ladakh_2021&lt;/code&gt; for places (&lt;code&gt;GPE&lt;/code&gt;=geopolitical entities). Run this from the PARENT directory of ladakh_2021&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; docanalysis --project_name ladakh_2021 --entities GPE --spacy_model spacy  --output gpe.csv 
&lt;/code&gt;&lt;/pre&gt;
</content>
	</entry>
	
	<entry>
		<title>FORCE11 Hackathon Kickoff -- Climate Knowledge Hunt Hackathon FSCI Edition 2023</title>
		<link href="/p/en/posts/fsci_blog/"/>
		<updated>2023-07-30T00:00:00Z</updated>
		<id>/p/en/posts/fsci_blog/</id>
		<content type="html">&lt;p&gt;#climate-knowledge-hunt-hackathon #climate #fsci #fsci2023&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://semanticclimate.org/&quot;&gt;semanticClimate&lt;/a&gt; team is excited to host an asynchronous hackathon from July 31 to August 4, 2023, where everybody learns about climate and tech by participating in the event! Feel free to drop in anytime.&lt;/p&gt;
&lt;img src=&quot;/p/static/img/fsci.jpeg&quot; /&gt;
&lt;p&gt;&lt;a href=&quot;https://forms.gle/XS49fPxx2HSs1k1BA&quot;&gt;Register here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Join us on the FSCI Slack - channel &lt;a href=&quot;https://fsci2023.slack.com/archives/C05GTHT92HJ&quot;&gt;#hackathon-semanticclimate&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About the Event&lt;/h2&gt;
&lt;p&gt;Want to see how semantic tools can make climate knowledge in scientific reports more accessible?&lt;/p&gt;
&lt;p&gt;The hackathon will contribute to a semantic layer on top of the Intergovernmental Panel On Climate Change (IPCC) &lt;a href=&quot;https://www.ipcc.ch/report/sixth-assessment-report-cycle/&quot;&gt;AR6 Synthesis Report: Climate Change 2023&lt;/a&gt; from its &lt;a href=&quot;https://www.ipcc.ch/assessment-report/ar6/&quot;&gt;Sixth Assessment Cycle&lt;/a&gt; from March 2023. The UN Secretary-General António Guterres described the report as a “how-to guide to defuse the climate time-bomb.”, but the report has barriers to its use — it’s complex, jargon-rich, and comes as a dumb PDF.&lt;/p&gt;
&lt;p&gt;In the hackathon, you will try the tools like &lt;a href=&quot;https://www.mediawiki.org/wiki/Wikibase&quot;&gt;Wikibase&lt;/a&gt; and Wikimedia tools to provide an interactive editing, query and display system for our #semanticClimate knowledge base.&lt;/p&gt;
&lt;p&gt;The hackathon will have three key activity tracks led by #semanticClimate team members that follow dedicated topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cities, (Mahvish, Renu, Waheb)&lt;/li&gt;
&lt;li&gt;Mountains and Glaciers, (Shiwani) and&lt;/li&gt;
&lt;li&gt;Food Security (Yasin),&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition there will be a prototype track for making custom readers for city climate plans. This is organised in conjunction with FSCI class &lt;a href=&quot;https://force11.org/fsci/post/course-list-with-abstracts-2023/#e08&quot;&gt;E08 – Publishing from Collections&lt;/a&gt; (Simon).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://semanticclimate.org/city-climate-plans/&quot;&gt;IPCC Reports and City Climate Change Plans: A Proof of Concept Prototype&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Key goals of the hackathon are improvement of the semantification of IPCC Report chapters related to the topics covered in the hackathon: Cities, Mountains and Glaciers, and Food Security; and the prototype mockup of ‘IPCC Reports and City Climate Change Plans’ project.&lt;/p&gt;
&lt;p&gt;Read &lt;a href=&quot;https://semanticclimate.org/p/en/posts/why-climate-knowledge-hunt/&quot;&gt;our blog&lt;/a&gt; to know more about the motivation behind running the hackathon.&lt;/p&gt;
&lt;h2&gt;How to take part&lt;/h2&gt;
&lt;p&gt;Find details on the &lt;a href=&quot;https://fsci2023.sched.com/&quot;&gt;FSCI Sched&lt;/a&gt;, join us on the FSCI Slack - channel &lt;a href=&quot;https://fsci2023.slack.com/archives/C05GTHT92HJ&quot;&gt;#hackathon-semantic climate&lt;/a&gt;.
The hackathon is asynchronous so you don&#39;t have to be present or have to present (unless you want to). Participants are welcome to drop in at any time for as little or long as they want.&lt;/p&gt;
&lt;h3&gt;Schedule&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;31 July: Opening session;&lt;/li&gt;
&lt;li&gt;1-3 August: asynchronous hackathon (join us on FSCI Slack);&lt;/li&gt;
&lt;li&gt;4 August: Closing session.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tasks for participants (optional)&lt;/h3&gt;
&lt;p&gt;Participants can choose tasks that suit them. If you have a special expertise or passion we can add to the example list below.&lt;/p&gt;
&lt;p&gt;All tasks, hackathon stages and workflows, as well as any software guides can be found linked from the documentation section.&lt;/p&gt;
&lt;p&gt;Each track — the topics: Cities, Mountains, and Food, as well as the custom reader for city climate plans — is led by a team member. Participants can join tracks and learn about the different tasks or run code themselves if they like. All tracks have the same core workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An IPCC chapter is selected to work on and teams browser and discuss contents&lt;/li&gt;
&lt;li&gt;Chapters are converted from PDF to HTML&lt;/li&gt;
&lt;li&gt;Machine learning are run on chapters to annotate them&lt;/li&gt;
&lt;li&gt;Dictionaries are automatically extracted and then refined through group discussion&lt;/li&gt;
&lt;li&gt;Semantically enriched content is outputted and stored on GitHub&lt;/li&gt;
&lt;li&gt;The team reviews progress and gives feedback to SC organisers&lt;/li&gt;
&lt;li&gt;Bibliographic analysis&lt;/li&gt;
&lt;li&gt;Conversion of large corpora to semantic formatting&lt;/li&gt;
&lt;li&gt;Your ideas and suggestions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The participants will be split into smaller teams, each with a theme – cities, mountains, and food, and #sC team member. The themes may be technical or related to climate topics. Some thought-out ones are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Climate dictionaries with Wikidata links&lt;/li&gt;
&lt;li&gt;Knowledge Graph – create Markdown files that can be read by Obsidian&lt;/li&gt;
&lt;li&gt;Structure of IPCC Reports – help us work out which bits to read!&lt;/li&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Goals, outcomes, and learning activities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Testing and developing tech&lt;/li&gt;
&lt;li&gt;Semantic collection of semantic facts that can be read by fifteen year olds and citizen with poor eyesight, and support local initiatives such as city climate plans&lt;/li&gt;
&lt;li&gt;Improvement of semantification of IPCC Report chapters related to our topics covered in the hackathon: Cities, Mountains and Glaciers, and Food Security,&lt;/li&gt;
&lt;li&gt;Prototype mockup of ‘IPCC Reports and City Climate Change Plans’&lt;/li&gt;
&lt;li&gt;Documentation improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What you’ll need to take part in the hackathon?&lt;/h3&gt;
&lt;p&gt;Documentation: Programme of activity and software installation step-by-step guides.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://semanticclimate.org/install-guide/&quot;&gt;Installation Guide&lt;/a&gt;: Step-by-step guide&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://semanticclimate.org/user-guide/&quot;&gt;User Guide&lt;/a&gt;: Hackathon programme of activity, stages, and workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Resources and links&lt;/h3&gt;
&lt;p&gt;GitHub repository: &lt;a href=&quot;https://github.com/petermr/semanticClimate&quot;&gt;https://github.com/petermr/semanticClimate&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;semanticClimate and the Hackathon&lt;/h2&gt;
&lt;p&gt;A significant part of any research effort is ‘Review of literature’, a task that often begins and closes by reading a few papers without contextualizing one’s own work in terms of all the existing knowledge in the field. Currently information around the world is largely “published” as monolithic documents ( PDF format) that further prevent one from ingesting or analyzing the work that has ever been conducted in a given field. Our emerging story is that we are catalyzing the building of a Global Semantic Knowledge Commons for Climate Change (GSKC), a self-improving network of semantic tools. In this Hackathon participants will use Open Source text and data mining tools developed by us in #semanticClimate to unlock the information trapped in the UN IPCC Climate Change reports. Our aim is to use PDF documents and release crucial scientific knowledge trapped in these by enriching them with annotations linked to #Wikidata — it makes the data accessible and understandable to everyone, be they citizens, scientists, datatech enthusiasts, or machines. We also hope to enable active conversations on Climate Data Access &amp;amp; Just Transitions, while addressing and finding real solutions by extracting Climate Knowledge together. &lt;strong&gt;The general goal is to show how humans and machines can create useful re-usable knowledge; and to make friends!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Hackathon will Unlock and bring Science to Citizens via Open Source Technology, and we have quite successfully done this over the past two years in collaboration with like minded Climate Enthusiasts among you! You are welcome to Bring your questions and/or knowledge of Climate to the Hackathon! IT experience (e.g., creating web pages) is valuable but NOT required; Willingness and excitement to work in small teams would be wonderful!&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Event Report | Climate Knowledge Hunt Hackathon May 2023</title>
		<link href="/p/en/posts/climate_knowledge_hunt_event_may_23_report/"/>
		<updated>2023-05-27T00:00:00Z</updated>
		<id>/p/en/posts/climate_knowledge_hunt_event_may_23_report/</id>
		<content type="html">&lt;p&gt;#semanticClimate Team and Gitanjali Yadav&#39;s Computational Biology Lab hosted Climate Knowledge Hunt Hackathon on 2023-05-19 at NII Board Room. We had about 20 in-person and 7 online participants.&lt;/p&gt;
&lt;h2&gt;Hack session&lt;/h2&gt;
&lt;img src=&quot;/p/static/img/semanticClimate_breakout_01.jpeg&quot; /&gt;
&lt;img src=&quot;/p/static/img/semanticClimate_breakout_03.jpeg&quot; /&gt;
</content>
	</entry>
	
	<entry>
		<title>Why are we running this hackathon?</title>
		<link href="/p/en/posts/why-climate-knowledge-hunt/"/>
		<updated>2023-04-18T00:00:00Z</updated>
		<id>/p/en/posts/why-climate-knowledge-hunt/</id>
		<content type="html">&lt;p&gt;The UN&#39;s IPCC AR6 report is probably the most important global document of this century. It gives&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the scientific basis of climate change&lt;/li&gt;
&lt;li&gt;its current and predicted effects&lt;/li&gt;
&lt;li&gt;how we can reduce the  harmful ones (mitigation)&lt;/li&gt;
&lt;li&gt;how we can adapt to them (adaptation)
As citizens of this planet we must learn as mush as we can and take responsible action.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Interpreting the report&lt;/h2&gt;
&lt;p&gt;The report is about 10,000 pages, very dense, and written for specialists and politicians to read, &lt;em&gt;and
agree&lt;/em&gt;. It&#39;s the basis of government policies and every word has been carefully drafted. Think of it as a
contract between nations.
It is primarily written for policy makers, not general citizens, and their language is jargon-rich (
because they must be precise) and full of statements based on evidence. This makes casual reading hard.
We believe it&#39;s essential to make this report accessible to citizens, and we believe our approach can
be used by committed high-school students (think Greta Thunberg!). We are not climate scientists so we are
not adding material, but rather providing reading and comprehension aids. We make the reports semantic.&lt;/p&gt;
&lt;h1&gt;semantic climate reports&lt;/h1&gt;
&lt;p&gt;The reports (ca 10,000 pages) are published in English and formatted in PDF. This makes it almost impossible to re-use (edit, translate hyperlink, etc.). Blind readers or non-anglophones (except in a few translations) will find it almost impossible. So we are making this semantic - machine-interpretable. Semantic means that machines can understand the structure of  the documents and can add meaning to parts. (This is not AI - we&#39;ll come to that later.)
We therefore transform the PDF into HTML, deduce the structure (chapters, tables, captions, etc.) and give the (sub)sections identifiers.
We can then add human meaning using ontologies (dictionaries), especially as jargon- and abbreviation busters. (e.g. &amp;quot;GHG&amp;quot;, &amp;quot;AR6/WGII&amp;quot;,
&amp;quot;nitrous oxide&amp;quot;, &amp;quot;palaeoarctic zone&amp;quot;).
We can also split the reports into smaller chunks and filter those with a common theme (e.g. &amp;quot;oceans&amp;quot;, &amp;quot;SouthAsia&amp;quot;, &amp;quot;cities&amp;quot;). To do this
we provide Open Source tools which anyone can use and also modify.
Note that the semantic process is completely transparent and understandable. Unlike AI/ML (which is often a black box), semantics can be designed
and manipulated at all levels of granularity.&lt;/p&gt;
&lt;h1&gt;Why not use AI/LLMs, etc.?&lt;/h1&gt;
&lt;p&gt;There&#39;s no doubt that LLMs will transform how we manage information and transform it into communal knowledge and decision making. But it&#39;s at an early stage and errors are frequent. (ChatGPT told PMR that he had studied at the University of Chicago!). LLMs are not transparent - where did it get that info? - and so they aren&#39;t reliable. Accuracy and traceability are &lt;em&gt;essential&lt;/em&gt; if our semantic version is going to be valuable.
Also the LLMs are run by closed, opaque companies. They almost certainly build in inequity (e.g. Anglophone, GlobalNorth), and without governance their motivation can&#39;t be trusted. So we believe that it will be better to build public Open LLMs, and semantic climate can be a useful input into that. (We can discuss at the hackathon!)&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Join us Today!</title>
		<link href="/p/en/posts/volunteers-wanted/"/>
		<updated>2022-11-07T00:00:00Z</updated>
		<id>/p/en/posts/volunteers-wanted/</id>
		<content type="html">&lt;h2&gt;Planet-saving information is a terrible thing to waste&lt;/h2&gt;
&lt;p&gt;If we are to hold any hope of mitigating the effects of &lt;a href=&quot;https://www.wikidata.org/wiki/Q112192791&quot;&gt;greenhouse gas emissions&lt;/a&gt; on our planet and its inhabitants, we must ensure that the latest scientific, technical and socio-economic knowledge on climate change is not just widely available, but immediately comprehensible to politicians, industrialists, educators, citizens, and scientists alike.&lt;/p&gt;
&lt;p&gt;Our most important and up-to-date resource, the &lt;a href=&quot;https://www.ipcc.ch/assessment-report/ar6/&quot;&gt;UN IPCC AR6 Climate Assessment Report&lt;/a&gt;, is a &lt;strong&gt;50-chapter, 10,000 page monster&lt;/strong&gt; filled to the brim with climate-science information and data...and all of it trapped in the &lt;a href=&quot;https://wiki.c2.com/?PdfSucks&quot;&gt;inflexible PDF format&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;The medium is the monkey wrench&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;As if the sheer volume and complexity of the information weren&#39;t enough, the &lt;a href=&quot;https://wiki.c2.com/?PdfSucks&quot;&gt;PDF format&lt;/a&gt; itself throws a spanner in the works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Meaning is lost in the haze of acronyms, abbreviations and scientific jargon&lt;/strong&gt; densely populating these important documents. These documents set a very high bar for comprehension (and therefore, attention) from policy makers that should be championing the cause — let alone the coalition of willing laypeople needed to implement meaningful changes on a global scale. Throughout this document and others like it, such terms must be understandable, &lt;a href=&quot;https://www.wikidata.org/wiki/Q216681&quot;&gt;in situ&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;An untapped wealth of knowledge remains buried in the document&#39;s rich text, charts, graphs and images&lt;/strong&gt;. It must therefore be made machine-readable so that it can be mined, extracted, and exported to new and potentially planet-saving uses.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We need to not only remove these obstacles, we need to create entirely new possibilities! 
And that&#39;s where &lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;&lt;em&gt;YOU&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; come into the picture!
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;The &lt;em&gt;#semanticClimate&lt;/em&gt; task force needs YOU!!&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://semanticclimate.github.io/p/en/posts/oaweek_getting_started/&quot;&gt;#semanticClimate&lt;/a&gt; is an industrious, international, multi-talented group of young scientists of diverse fields. Together, we are developing a FREE open-access &lt;a href=&quot;http://www.semantictoolkit.org&quot;&gt;toolkit&lt;/a&gt; that releases climate-related knowledge from PDF prison.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;OUR&lt;/strong&gt; MISSION&lt;/h2&gt;
&lt;p&gt;We transform and enrich climate-related documents to make them understandable, useful, and accessible to anyone and everyone— be they citizens, scientists, or policy-makers.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;YOUR&lt;/strong&gt; &lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;MISSION&lt;/a&gt; &lt;em&gt;(should you choose to accept it...)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Join us! Apply your skills!&lt;/strong&gt; Be a part of building and enhancing powerful new tools that take information and transforms it into new, structured, filtered, and actionable knowledge!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In alpha-tests, our toolkit is successfully:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;converting PDFs&lt;/strong&gt; into editable and accessible HTML, XML, and CSV;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;extracting important keywords and concepts&lt;/strong&gt; from the text (including named entities, scientific terms, acronyms, and abbreviations);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;creating specialized dictionaries and glossaries&lt;/strong&gt; linking those terms to wikidata entries containing their definition; and,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;semantically enriching the converted HTML with annotations linked to the wikidata&lt;/strong&gt; so that anyone can simply click on, or mouse-over an unknown term to learn their meaning, thus making comprehension of these critically important documents available to everyone — even machines!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;And this is just the beginning!&lt;/em&gt;&lt;/strong&gt; Imagine what will be possible with &lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;&lt;em&gt;YOU&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; on our team!
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1&gt;Looking for a meaningful challenge?&lt;/h1&gt;
&lt;h2&gt;Join our coding crew!&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;We&#39;re looking for smart-thinking, enthusiastic volunteers&lt;/strong&gt; keen on helping us transform climate science and policy information into action. Are &lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;you or someone you know&lt;/a&gt; one of them?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We&#39;re looking for:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;CODERS&lt;/strong&gt;&lt;/a&gt; skilled in python, HTML&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;CLIMATE SCIENTISTS&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;TERMINOLGISTS&lt;/strong&gt; and &lt;strong&gt;ONTOLOGISTS&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;TRANSLATORS&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt; &lt;strong&gt;WIKIMEDIANS&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt; &lt;strong&gt;WEBSITE DEVELOPERS&lt;/strong&gt;&lt;/a&gt; who can build and maintain (mainly static) sites&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;COMMUNICATIONS PLATFORM ARCHITECTS/MANAGERS&lt;/strong&gt;&lt;/a&gt; to help develop and attract people to interrelated networks of communities and facilitate participation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contact us!&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;Introduce yourself&lt;/a&gt; to us, and we&#39;ll introduce you to the world of difference you can make — given the right opportunity!&lt;/p&gt;
&lt;h1&gt;&lt;a href=&quot;mailto:semanticclimate+volunteer@gmail.com?subject=I want to be a #semanticClimate Volunteer!&amp;body=I want to volunteer for the #semanticClimate task force!&quot;&gt;&lt;strong&gt;Just click here to begin.&lt;/strong&gt;&lt;/a&gt;&lt;/h1&gt;
</content>
	</entry>
	
	<entry>
		<title>Getting started with semanticClimate</title>
		<link href="/p/en/posts/oaweek_getting_started/"/>
		<updated>2022-10-24T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_getting_started/</id>
		<content type="html">&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;If you&#39;ve just landed on #semanticClimate and are keen to get started here are
some approaches&lt;/p&gt;
&lt;h2&gt;Asynchronous&lt;/h2&gt;
&lt;h3&gt;entry link&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://semanticclimate.github.io/p/en/events/formats-for-future-oa-week-2022/&quot;&gt;https://semanticclimate.github.io/p/en/events/formats-for-future-oa-week-2022/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;link to Notebooks&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://semanticclimate.github.io/p/en/posts/climate-knowledge-hunt/&quot;&gt;https://semanticclimate.github.io/p/en/posts/climate-knowledge-hunt/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;list of URLs coming soon&lt;/p&gt;
&lt;h3&gt;link to videos&lt;/h3&gt;
&lt;p&gt;coming soon.&lt;/p&gt;
&lt;h3&gt;Etherpad&lt;/h3&gt;
&lt;p&gt;We have created an Etherpad and if you have registered you will have received its URL. You can leave
questions or suggestions and people will answer as soon as they visit the pad.&lt;/p&gt;
&lt;h2&gt;Synchronous meetings&lt;/h2&gt;
&lt;p&gt;If you&#39;ve registered: Every day 2022-10-24:28 meet 1230 UTC at
&lt;a href=&quot;https://zoom.us/j/96741193544?pwd=T1Ivb1Y5YmdvM3cwc2x1VjVUQ2ZVdz09&quot;&gt;https://zoom.us/j/96741193544?pwd=T1Ivb1Y5YmdvM3cwc2x1VjVUQ2ZVdz09&lt;/a&gt;
wait to be let in.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Workflow for semanticClimate</title>
		<link href="/p/en/posts/oaweek_workflow/"/>
		<updated>2022-10-23T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_workflow/</id>
		<content type="html">&lt;h1&gt;Steps in processing a Chapter&lt;/h1&gt;
&lt;p&gt;We assume&lt;/p&gt;
&lt;h2&gt;PDF2HTML&lt;/h2&gt;
&lt;p&gt;The raw chapter is in PDF which requires messy heuristic processing into HTML. We have done this for a small
number of chapters and will be automating it as soon as we can. Here&#39;s a typical chunk.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./assets/chap02.png&quot; /&gt; &lt;!-- check this --&gt;&lt;/p&gt;
&lt;p&gt;There are no words, lines, paragraphs in PDF. The tools usually guess right but here there&#39;s a problem of the
top and left margins. We have to set clipping boxes. Sometimes these haven&#39;t worked. But generally we can get HTML
(although things like font styles, subscripts, etc. are often trashed.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Explaining semantic</title>
		<link href="/p/en/posts/oaweek_semantic/"/>
		<updated>2022-10-23T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_semantic/</id>
		<content type="html">&lt;h2&gt;What&#39;s semantic?&lt;/h2&gt;
&lt;p&gt;&amp;quot;semantic&amp;quot; means that at some level machines can &amp;quot;understand&amp;quot; data (in this case text).&lt;/p&gt;
&lt;h3&gt;example:&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Two countries (China, India) contributed more than 50% to the net
6.5 GtCO2eqyr-1 increase in GHG emissions during 2010-2019 (at
39% and 14%, respectively), while ten countries (China, India,
Indonesia, Vietnam, Iran, Turkey, Saudi Arabia, Pakistan, Russian
Federation, Brazil) jointly contributed about 75% (Figure 2.9) (see
also Minx et al., 2021; Crippa et al., 2021).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a human you understand some of this and not other parts; by default a machine &amp;quot;understands&amp;quot; nothing. It sees
&lt;code&gt;&amp;quot;T&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;o&amp;quot; ...&lt;/code&gt;
(no words, no sopaces, no newlines and no understanding of the)
We need to give it rules (or models) that give it the power to parse this into objects it can process.&lt;/p&gt;
&lt;h3&gt;syntax&lt;/h3&gt;
&lt;p&gt;Unfortunately the reports are only available (2022-10) in PDF. Unlike HTML this is very unpleasant to process
and introduces errors. PDF2HTML is the weakest part of our toolchain. If the authors could make their text-processor
output available it would be a huge help.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NLTK&lt;/code&gt; and &lt;code&gt;spaCy&lt;/code&gt; and other tools have methods for parsing sentences and extracting the syntax. Here&#39;s
an online part of speeh tagger and chunker &lt;a href=&quot;https://www.link.cs.cmu.edu/cgi-bin/link/construct-page-4.cgi#submit&quot;&gt;parser from CMU site&lt;/a&gt; you can try it!
It gives:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(S (NP (NP Two countries)
       (SBAR (S (NP China , India)
                (VP contributed more
                    (PP than
                        (NP 50))
                    (PP to
                        (NP (QP the net 6.5)
                            GtCO2eqyr-1))))))
   (VP increase
       (PP in
           (NP GHG emissions))
       (PP during
           (NP 2010))))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It picks out nouns/noun_phrases (NP)
&lt;code&gt;/Two countries/ /China , India/ /the net 6.5) GtCO2eqyr-1)/ /GHG emissions/ /2010/&lt;/code&gt;
(gets one wrong &lt;code&gt;increase&lt;/code&gt; is a noun not a verb). And verbs VP (&lt;code&gt;contributed&lt;/code&gt;), and prepositions (PP)
but has no idea what they mean.&lt;/p&gt;
&lt;h3&gt;general semantics&lt;/h3&gt;
&lt;p&gt;We concentrate on nouns and now add meaning to them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;Two countries&lt;/code&gt;&lt;/em&gt; : &lt;code&gt;NLTK&lt;/code&gt; knows about numbers and also that &lt;code&gt;countries&lt;/code&gt; is the plural of &lt;code&gt;country&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;China , India&lt;/code&gt;&lt;/em&gt; : &lt;code&gt;spaCy&lt;/code&gt; knows these are countries&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;6.5&lt;/code&gt;&lt;/em&gt; : &lt;code&gt;NLTK&lt;/code&gt; and &lt;code&gt;spaCy&lt;/code&gt; know is a floating point number&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;2010&lt;/code&gt;&lt;/em&gt; : the tools know this could be a date&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Wikidata semantics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;GHG&lt;/code&gt;&lt;/em&gt; : we can automatically look this up in Wikidata.org and get:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&quot;language-greenhouse&quot;&gt;as in an atmosgphere of a planet (usually Earth) that absorbs 
and emits radiation within the thermal infrared range and 
causes the greenhouse effect
48 statements, 81 sitelinks - 07:22, 15 October 2022

Marshfield Municipal Airport (Q650060) : GHG
airport
22 statements, 5 sitelinks - 05:58, 3 October 2022

etc.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wikidata has several entries for &lt;code&gt;GHG&lt;/code&gt;. The top one is what we want, with an &lt;code&gt;identifier&lt;/code&gt; of
&lt;code&gt;Q167336&lt;/code&gt; . This illustrates &lt;em&gt;ambiguity&lt;/em&gt; - common words and acronyms/abbreviations often have multiple meanings.
In this case  we could see if the nouns in the &lt;code&gt;description&lt;/code&gt; (&amp;quot;gas&amp;quot;, &amp;quot;atmosphere&amp;quot;, &amp;quot;planet&amp;quot;, &amp;quot;radiation&amp;quot;...)
are common in the text of the report. That&#39;s one role of our &lt;code&gt;dictionaries&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;report semantics&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;GtCO2eqyr-1&lt;/code&gt; occurs frequently and we can collect these sorts of terms. However they need human interpretation (this is badly
formatted and should read
GtCO&lt;sub&gt;2&lt;/sub&gt;eq yr&lt;sup&gt;-1&lt;/sup&gt;. It reads &amp;quot;gigatonnes of carbon dioxide equivalents per year&amp;quot;. We also tackle this
by creating it as an entry (or synonym) in a climateChange dictionary)&lt;/p&gt;
&lt;h3&gt;hyperlinks&lt;/h3&gt;
&lt;p&gt;Hyperlinks are assumed, not explicit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(Figure 2.9)&lt;/code&gt; is a link to Figure 2.9 (which is not explicitly semantically labelled)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(see also Minx et al., 2021; Crippa et al., 2021).&lt;/code&gt; are 2 links to the bibliography (again needs messy
heuristics)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;argumentation&lt;/h3&gt;
&lt;p&gt;We&#39;d like to extract more meaning than just the nouns (&amp;quot;which countries contributed most to the emissions&amp;quot;). There
are research tools that can address this but they are problem-specific.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Searching for climate knowledge</title>
		<link href="/p/en/posts/oaweek_search/"/>
		<updated>2022-10-22T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_search/</id>
		<content type="html">&lt;h2&gt;OA Week 2022: searching for climate knowledge&lt;/h2&gt;
&lt;p&gt;There several ways of searching for knowledge. Prepared navigation (e.g. book, movie) is one but is reader-dependent; we concentrate on:&lt;/p&gt;
&lt;h3&gt;natural language processing (NLP)&lt;/h3&gt;
&lt;p&gt;This uses mixtures of sets of words (dictionaries), linguistic rules and machine learning to analyse
documents. NLP is increasingly multilanguage and has rules for parts of speech, compound words,
diacritics, inflexions sentence structure and much more.&lt;/p&gt;
&lt;h3&gt;keyword (supervised) search&lt;/h3&gt;
&lt;p&gt;The commonest way of searching (Google, Bing, etc.). It usually relies on the content being text, or indexed into text. For specific terms (e.g. names,
scientific objects) it can be precise but for general terms (e.g. &amp;quot;justice&amp;quot;) it can be very broad. Often the precise term doesn&#39;t occur in the
article and we need synonyms (e.g. &amp;quot;nitrous oxide&amp;quot; is a synonym for &amp;quot;N2O&amp;quot;).
We tackle this by creating a large expandable set of &lt;strong&gt;dictionaries&lt;/strong&gt; with precise terms, synonyms and translations which are used for &lt;strong&gt;supervised&lt;/strong&gt; search. We use NLP tools and Wikidata to help precision (e.g. &lt;em&gt;stemming&lt;/em&gt; and &lt;em&gt;lexemes&lt;/em&gt;)&lt;/p&gt;
&lt;h3&gt;unsupervised search&lt;/h3&gt;
&lt;p&gt;When we don&#39;t know what the content is like or what we should search for, we use &lt;strong&gt;unsupervised&lt;/strong&gt;
search. &amp;quot;Tell me the most important terms in this document&amp;quot;, &amp;quot;which documents are most similar to
each other&amp;quot;. We use NLP tools to identify tokens (words, phrases and sentences) and parse them (e.g. finding parts
of speech (PoS tagging)). A common ML approach is to measure the lexical or linguistic distance (similarity) between
tokens and create clusters of tokens. (To understand clustering create an &lt;a href=&quot;https://openknowledgemaps.org&quot;&gt;OpenKnowledgeMap&lt;/a&gt; which
analyses Open Access documents).&lt;/p&gt;
&lt;h3&gt;Toolkits and material&lt;/h3&gt;
&lt;p&gt;Over the last 10-20 yeards there has been a huge increase in cutting-edge Open NLP tools: wordlists, parsers, ML tools.
These exist in most common languages (Java, C++, R); here we use Python which is very comprehensive (and often
wraps the C++ or Java tools). Most of the functionality is in &lt;code&gt;nltk&lt;/code&gt; or &lt;code&gt;spaCy&lt;/code&gt; and these can be automatically
called from our tool &lt;code&gt;docanalysis&lt;/code&gt; (created by Shweata N Hegde). (Think of &lt;code&gt;nltk&lt;/code&gt; , &lt;code&gt;spaCy&lt;/code&gt; as components (gearbox, wheels) and
&lt;code&gt;docanalysis&lt;/code&gt; as a finished vehicle). There are more than 50 components in our tools all of which are automatically
loaded/installed into your machine or Colab. These components provide different ways of searching; an example is abbreviations&lt;/p&gt;
&lt;h3&gt;abbreviations and acronyms&lt;/h3&gt;
&lt;p&gt;One of the most important places to add semantics is acronyms, abbreviations or initialism (we&#39;ll crudely use &amp;quot;abbreviation&amp;quot;
from here). What&#39;s &amp;quot;AFOLU&amp;quot;? There are several ways of automating this partly of fully.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;many documents have an explicit list of acronyms at the start or end&lt;/li&gt;
&lt;li&gt;syntax such as
&lt;code&gt;GHG (greenhouse gas)&lt;/code&gt;
&lt;code&gt;greenhouse gas (GHG)&lt;/code&gt;
implies the linkage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tools such as &lt;code&gt;spaCy&lt;/code&gt; have rules and models to extract &lt;code&gt;abbreviation / meaning&lt;/code&gt; pairs&lt;/p&gt;
&lt;h4&gt;Wikipedia lookup&lt;/h4&gt;
&lt;p&gt;Wikipedia contains a lot of disambiguation pages for abbreviations. Since WP covers all disciplines, we need to
&lt;em&gt;disambiguate&lt;/em&gt; the hits. This is heuristic and can use the description in the page (to see if the words relate to
the discipline.).&lt;/p&gt;
&lt;h4&gt;Wikidata lookup&lt;/h4&gt;
&lt;p&gt;Wikidata can lookup the initialism and return a list of hits. These usually have descriptions which agains can be
matched for relevance.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Introduction to semanticClimate</title>
		<link href="/p/en/posts/oaweek_intro/"/>
		<updated>2022-10-22T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_intro/</id>
		<content type="html">&lt;h1&gt;OA Week 2022: Introduction to #semanticClimate&lt;/h1&gt;
&lt;p&gt;We are really excited to offer semanticClimate for OA week. It&#39;s a relatively new way of communicating complex topics and
it&#39;s best understood by actually &amp;quot;learning by doing&amp;quot;. Don&#39;t be frightened by &amp;quot;semantic&amp;quot;; it means using machines to help
with the bits machines are good at (&lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_Web&quot;&gt;TimBl&#39;s semantic web&lt;/a&gt;), freeing human brains to do what they are best at.&lt;/p&gt;
&lt;p&gt;&amp;quot;Climate&amp;quot; is complex! No human understands everything, so it&#39;s critical to link different resources: data, text, simulations, etc. &amp;quot;semantic&amp;quot; means they can all talk to each other. Machine&amp;lt;-&amp;gt;machine, human&amp;lt;-&amp;gt;machine and human&amp;lt;-&amp;gt;human. And anyone can be part of this; if you have resource that could be useful join us this week! There&#39;s so many levels of knowledge and so many subdisciplines&lt;/p&gt;
&lt;p&gt;Our approach can be adjusted for any climate approach, but our examples here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &amp;quot;Mitigation&amp;quot; &lt;a href=&quot;https://www.ipcc.ch/report/ar6/wg3/&quot;&gt;section (WG3) of the IPCC AR6 report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Novel searching of the OpanAccess literature (with &lt;code&gt;pygetpapers&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Example: &amp;quot;what does &#39;GHG&#39; mean and why does it matter?&amp;quot;&lt;/h2&gt;
&lt;p&gt;Many of you will know this means &amp;quot;Greenhouse Gas&amp;quot; in a climate context, but do you know what these gases actually are?
To answer this we rely heavily on Wikimedia: Wikipedia and Wikidata. You can think of Wikidata as the &amp;quot;data items in Wikipedia articles&amp;quot; (though it&#39;s much more than that), and this week (2022-10-20) it passed 100*000_000 items! Here&#39;s the &lt;a href=&quot;https://www.wikidata.org/wiki/Q167336&quot;&gt;wikidata item for &amp;quot;greenhouse gas&amp;quot;&lt;/a&gt;. It&#39;s multilingual (&lt;a href=&quot;https://hi.wikipedia.org/wiki/%E0%A4%97%E0%A5%8D%E0%A4%B0%E0%A5%80%E0%A4%A8%E0%A4%B9%E0%A4%BE%E0%A4%89%E0%A4%B8*%E0%A4%97%E0%A5%88%E0%A4%B8&quot;&gt;ग्रीनहाउस गैस&lt;/a&gt; and we&#39;ve already had contributions to our notebooks in 7 languages)&lt;/p&gt;
&lt;h1&gt;How this week will work&lt;/h1&gt;
&lt;p&gt;This is an experiment! Think &amp;quot;us&amp;quot;, not &amp;quot;I&amp;quot; and &amp;quot;you&amp;quot;. We have working examples of search and analysis tools and also content (IPCC) with videos and text. The week is largely asynchronous; you can try these in your own timezone at your own pace. There are discussion tools so you can leave questions or offer content. We&#39;ll aim to have a time which most people can access (probably 1230 UTC, where we normally meet with India and Americas) for an hour or so on Zoom. We stress &amp;quot;learning by doing&amp;quot; and also constantly evolving software, documentation, content, examples. These work &amp;quot;most of the time&amp;quot; but may depend on experience and location (&lt;a href=&quot;https://en.wikipedia.org/wiki/Perpetual_beta&quot;&gt;Perpetual beta&lt;/a&gt;); if you have problems, then identify them, and if you can, help to fix them.&lt;/p&gt;
&lt;p&gt;We expect that the content and approach will change a lot during the week.&lt;/p&gt;
&lt;p&gt;Typical journeys might be:&lt;/p&gt;
&lt;h2&gt;term extraction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;watch video on term extraction&lt;/li&gt;
&lt;li&gt;try the Colab/Jupyter notebook on Chapter02 (tutorial)&lt;/li&gt;
&lt;li&gt;have a question, so post on discussion list/Etherpad&lt;/li&gt;
&lt;li&gt;then try Notebook on Chapter11 (experiment!)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;creating dictionaries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;watch video on creating dictionaries&lt;/li&gt;
&lt;li&gt;watch video on markup using dictionaries&lt;/li&gt;
&lt;li&gt;try Colab/Jupyter Notebook on Chapter02 with prepared dictionaries&lt;/li&gt;
&lt;li&gt;try on Chapter02 with &lt;code&gt;country.xml&lt;/code&gt; (experiment!)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;search OA literature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;search for &lt;code&gt;climate justice&lt;/code&gt; in EuropePMC using &lt;code&gt;pygetpapers&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;automatically download (say) 50 papers.&lt;/li&gt;
&lt;li&gt;skim-read to determine the ones you might be interested in&lt;/li&gt;
&lt;li&gt;refine query (dates, Boolean queries, etc.)&lt;/li&gt;
&lt;li&gt;repeat&lt;/li&gt;
&lt;li&gt;analyse results with &lt;code&gt;docanalysis&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Suggested reading of blog posts&lt;/h1&gt;
&lt;h2&gt;&lt;a href=&quot;oaweek_communication.md&quot;&gt;Communication&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This outines how we work during OAWeek&lt;/p&gt;
&lt;h2&gt;oaweek_intro.md&lt;/h2&gt;
&lt;p&gt;This file&lt;/p&gt;
&lt;h2&gt;&lt;a href=&quot;./oaweek_semantic.md&quot;&gt;Semantic&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Explains &amp;quot;semantic&amp;quot; and why we are doing this. Read early.&lt;/p&gt;
&lt;h2&gt;&lt;a href=&quot;./oaweek_search.md&quot;&gt;Search&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Explains what the most important tools do. Also watch the videos&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Communication within semanticClimate</title>
		<link href="/p/en/posts/oaweek_communication/"/>
		<updated>2022-10-22T00:00:00Z</updated>
		<id>/p/en/posts/oaweek_communication/</id>
		<content type="html">&lt;h1&gt;Basis of collaboration&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;#semanticClimate&lt;/code&gt; is essentially a bottom-up, volunteer, meritocratic project. We share the need to create
working systems where different people contribute a variety of tools and functionality. This is common in OpenSource
projects but requires communal discipline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we need to know what each of us is doing&lt;/li&gt;
&lt;li&gt;we can help each other find and overcome problems&lt;/li&gt;
&lt;li&gt;we need to know that the pieces fit together and work properly.&lt;/li&gt;
&lt;li&gt;we need to interact with the outside world, especially those who want to use the tools&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We communicate both &lt;em&gt;synchronously&lt;/em&gt; (2 or more people talking in a group) and &lt;em&gt;asynchronously&lt;/em&gt; where we leaves messages
that can be answered, re-answered, etc.&lt;/p&gt;
&lt;p&gt;This is not trivial. The world has created a variety of tools and here are the ones we have found most useful.
Access rights are always a problem (we have been Zoom-bombed; not fun).&lt;/p&gt;
&lt;h1&gt;Asynchronous&lt;/h1&gt;
&lt;h2&gt;Open Notebook Science&lt;/h2&gt;
&lt;p&gt;As far as possible everything we do is available to the outside world as soon as it is done and is permanent.
See &lt;a href=&quot;https://en.wikipedia.org/wiki/Open-notebook_science&quot;&gt;Open Notebook Science&lt;/a&gt;. The methods we use include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Github repositories (mainly code and documents)&lt;/li&gt;
&lt;li&gt;Github Issues (mainly for problems, e.g. bugs)&lt;/li&gt;
&lt;li&gt;Jupyter/Colab Notebooks (mainly for learning and simple prototyping)&lt;/li&gt;
&lt;li&gt;Github Discussions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;openVirus Slack&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;(openVirus is a historic name where semanticCliamte is discussed)&lt;/em&gt;
Slack is neither permanent nor public. We use it internally for rapid ephemeral messages but the unpaid Slack dies
after ca. 2 months. If you want to join the project more permanently we can add your email.&lt;/p&gt;
&lt;h2&gt;Etherpad&lt;/h2&gt;
&lt;p&gt;This is a very ephemeral shared (text) pad without structure that &lt;em&gt;anyone&lt;/em&gt; can write on. It preserves history (character-by-character).
It uses security-by-obscurity (i.e. the URL is not discoverable by chance) and is effortless to use. It&#39;s very useful for things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;interchanging URLs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;making ad hoc lists&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;introductions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text-cut-and-paste&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regard all content as public.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;We&#39;ll open an Etherpad for the duration of the OpenAccess Week, and delete a few days after&lt;/strong&gt;&lt;/em&gt;.
If you want something, copy it somewhere safe.&lt;/p&gt;
&lt;h1&gt;Synchronous&lt;/h1&gt;
&lt;p&gt;We use Skype, Teams (when we have to) but for OAWeek we only use:&lt;/p&gt;
&lt;h2&gt;Zoom&lt;/h2&gt;
&lt;p&gt;We&#39;ve used many tools over the years, e.g. Discord, Gathertown (fun tool) but generally use Zoom in paid-for mode (University of Cambridge).
Sometimes we use &lt;em&gt;breakout rooms&lt;/em&gt; where separate conmversations can take place but for OAWeek we&#39;d plan to have just the
main room. Zoom Chat is ephemeral - dispappears after the session - (and tricky to copy). You&#39;ll need an invite from us.
PM-R is guaranteeing to be present at 1230 UTC each day. If there is a critucal mass of people at other times we can
open an impromptu session.&lt;/p&gt;
</content>
	</entry>
	
	<entry>
		<title>Climate Knowledge Hunt Hackathon Material</title>
		<link href="/p/en/posts/climate-knowledge-hunt/"/>
		<updated>2022-10-22T00:00:00Z</updated>
		<id>/p/en/posts/climate-knowledge-hunt/</id>
		<content type="html">&lt;p&gt;Rendered below is the Colab Notebook we prepared for the Hackathon. Scroll down to see the results.&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/ShweataNHegde/069453dae0db56c01cb4c78e5847842f.js&quot;&gt;&lt;/script&gt;
&lt;h2&gt;Highlights from Notebook&lt;/h2&gt;
&lt;p&gt;You&#39;ll find the code snippets and commands in the embedded Notebook. Here are some interesting tidbits.&lt;/p&gt;
&lt;h3&gt;Convert PDF to HTML using &lt;a href=&quot;https://github.com/petermr/docanalysis&quot;&gt;&lt;code&gt;py4ami&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;First step in making IPCC Reports semantic is to convert dumb PDF needs to HTML. &lt;code&gt;py4ami&lt;/code&gt; does the job for us. Here&#39;s the preview of converted HTML.&lt;/p&gt;
&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://htmlpreview.github.io/?https://github.com/petermr/semanticClimate/blob/main/ipcc/ar6/wg3/Chapter02/fulltext.html&quot; style=&quot;&quot; width=&quot;100%&quot; height=&quot;300px&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;h3&gt;Extract abbreviations using &lt;a href=&quot;https://github.com/petermr/docanalysis&quot;&gt;&lt;code&gt;docanalysis&lt;/code&gt;&lt;/a&gt; and write to an ami-dictionary&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;docanalysis&lt;/code&gt; automatically extracts abbreviations, their full forms, and potential Wikidata IDs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;entry name=&amp;quot;VRE&amp;quot; term=&amp;quot;Variable Renewable Energy&amp;quot; wikidataID=&amp;quot;[&#39;//www.wikidata.org/wiki/Q7915732&#39;]&amp;quot;/&amp;gt;
  &amp;lt;entry name=&amp;quot;SDGs&amp;quot; term=&amp;quot;sustainable development goals&amp;quot; wikidataID=&amp;quot;[&#39;//www.wikidata.org/wiki/Q7649586&#39;]&amp;quot;/&amp;gt;
  &amp;lt;entry name=&amp;quot;TPES&amp;quot; term=&amp;quot;total primary energy  supply&amp;quot; wikidataID=&amp;quot;[]&amp;quot;/&amp;gt;
  &amp;lt;entry name=&amp;quot;TFC&amp;quot; term=&amp;quot;total final energy consumption&amp;quot; wikidataID=&amp;quot;[]&amp;quot;/&amp;gt;
  &amp;lt;entry name=&amp;quot;CSP&amp;quot; term=&amp;quot;Concentrating  solar  power&amp;quot; wikidataID=&amp;quot;[]&amp;quot;/&amp;gt;
  &amp;lt;entry name=&amp;quot;LIBs&amp;quot; term=&amp;quot;lithium-ion batteries&amp;quot; wikidataID=&amp;quot;[&#39;//www.wikidata.org/wiki/Q106988181&#39;]&amp;quot;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, &lt;a href=&quot;www.wikidata.org/wiki/Q7915732&quot;&gt;www.wikidata.org/wiki/Q7915732&lt;/a&gt; takes you to the Wikidata pages that tells you all about Variable Renewable Energy. The Wikidata page also points you to the Wikipedia page: &lt;a href=&quot;https://en.wikipedia.org/wiki/Variable_renewable_energy&quot;&gt;https://en.wikipedia.org/wiki/Variable_renewable_energy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Supervised search using dictionaries from other sources&lt;/h3&gt;
&lt;p&gt;Worcloud generated based on number of hits for terms in the dictionaries.
&lt;img src=&quot;/p/static/img/climate_terms.png&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;Annotate and index HTML version of IPCC reports using &lt;a href=&quot;https://github.com/petermr/pyami&quot;&gt;py4ami&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pyami&lt;/code&gt;, using abbreviation and other climate-related dictionary, can annotate the HTML version of IPCC Reports. &lt;a href=&quot;https://htmlpreview.github.io/?https://github.com/petermr/semanticClimate/blob/main/ipcc/ar6/wg3/Chapter02/annotated/fulltext_emissions.html&quot;&gt;Click here&lt;/a&gt; or check out the preview below.&lt;/p&gt;
&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://htmlpreview.github.io/?https://github.com/petermr/semanticClimate/blob/main/ipcc/ar6/wg3/Chapter02/annotated/fulltext_emissions.html&quot; style=&quot;&quot; width=&quot;100%&quot; height=&quot;300px&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
</content>
	</entry>
</feed>